{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Text generation models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "One very basic kind of text generation model is the Markov model.  In\n",
    "such a model, we have a state which consists of the previous character.\n",
    "We also have a matrix of transitions from one character to another.  We\n",
    "*train* the model by feeding it some text, and observing the\n",
    "transitions.  We can then generate more text from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "def train_sentence(transitions, sentence, depth=1):\n",
    "    # We need a \"special\" character to represent the beginning of a sentence.\n",
    "    # This is also the character we'll use to feed the generator, below.\n",
    "    prevchar = \"•\" * depth\n",
    "    for char in sentence:\n",
    "        transitions[prevchar][char] += 1\n",
    "        prevchar = prevchar[1:] + char\n",
    "\n",
    "    return transitions\n",
    "\n",
    "def split_text(text):\n",
    "    for sentence in re.finditer(\".*?([.?!][”’]?|\\n\\n)\", text, re.DOTALL):\n",
    "        # Turn all sequences of whitespace into a single space\n",
    "        sentence = re.sub(\"[ \\t\\n\\r]+\", \" \", sentence.group(0)).strip()\n",
    "        if sentence != '':\n",
    "            yield sentence\n",
    "\n",
    "def train(filename, depth=1):\n",
    "    transitions = defaultdict(lambda: defaultdict(int))\n",
    "    with open(filename) as fin:\n",
    "        text = fin.read()\n",
    "        for sentence in split_text(text):\n",
    "            if len(sentence) < 3:\n",
    "                continue\n",
    "            transitions = train_sentence(transitions, sentence, depth)\n",
    "\n",
    "    return transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def format_transitions(trs):\n",
    "    rows = []\n",
    "    for key in trs:\n",
    "        for key2 in trs[key]:\n",
    "            rows.append({'from': key, 'to': key2, 'n': trs[key][key2]})\n",
    "    data = pd.DataFrame(rows)\n",
    "    data = data.pivot_table(index='from', columns='to', values='n')\n",
    "    data = data.div(data.sum(axis=1), axis=0)\n",
    "    data[np.isnan(data)] = 0\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def produce(transitions):\n",
    "    if isinstance(transitions, defaultdict):\n",
    "        transitions = format_transitions(transitions)\n",
    "\n",
    "    # Nifty trick: auto-calculate the depth we were trained on\n",
    "    depth = len(transitions.index[0])\n",
    "\n",
    "    output = \"\"\n",
    "    last = \"•\" * depth\n",
    "    nxt = \"\"\n",
    "\n",
    "    while nxt not in [\".\", \"?\", \"!\"]:\n",
    "        trs = transitions.loc[last]\n",
    "        nxt = np.random.choice(trs.index, p=trs)\n",
    "        last = last[1:] + nxt\n",
    "        output += nxt\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "tr = train(\"alice.txt\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "[produce(tr) for x in range(5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Ideas for extension:\n",
    "\n",
    "- Train on a different text\n",
    "- Try normalizing the text in different ways (e.g. what happens if you take\n",
    "  out quotation marks?)\n",
    "- Play around with different depths.  Do different ones work better for\n",
    "  different texts?\n",
    "- Try generating longer passages (you will need to alter the training\n",
    "  also)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Hidden Markov Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hmmlearn.hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STATES = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below code, `alphabet` is the list of characters that appear in the text.  `emission_probs` is a matrix with `N_STATES` rows and `len(alphabet)` columns.  It is the initial estimate of the probability of each letter in each state.  As a \"dumb\" initial guess, we set all states to have the probability of each letter in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"alice.txt\") as fin:\n",
    "    text = fin.read()\n",
    "    \n",
    "alphabet = set(list(text))\n",
    "alphabet -= {\"\\n\"}\n",
    "alphabet = sorted(alphabet)\n",
    "\n",
    "frequencies = defaultdict(int)\n",
    "for char in text:\n",
    "    if char == '\\n':\n",
    "        continue\n",
    "    frequencies[char] += 1\n",
    "    \n",
    "total_chars = sum([x for x in frequencies.values()])\n",
    "    \n",
    "emission_probs = np.stack([np.fromiter([frequencies[x] / total_chars for x in alphabet], dtype=np.float64)] * N_STATES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to transform the characters in the text into numbers, because HMMs speak numbers.  We do this manually, but we could just as easily use `sklearn.preprocessing.LabelEncoder` for it.  We need to end up with a numpy vector, each element of which is a one-element vector containing a character-number (that's what the `.reshape()` incantation is for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(split_text(text))\n",
    "letter_vector = np.fromiter([alphabet.index(char) for sentence in sentences for char in sentence], dtype=np.int64).reshape(-1,1)\n",
    "lengths = list(map(len, sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(letter_vector) - sum(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm = hmmlearn.hmm.MultinomialHMM(n_components=N_STATES, algorithm=\"viterbi\", n_iter=100, verbose=True)\n",
    "\n",
    "hmm.emission_prob_ = emission_probs\n",
    "\n",
    "hmm.fit(letter_vector, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols, _states = hmm.sample(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''.join([alphabet[x[0]] for x in symbols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning options:\n",
    "- use words instead of letters\n",
    "- explore viterbi vs map algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Neural networks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "name": "presentation.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
